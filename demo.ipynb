{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "\n",
    "# System libs\n",
    "import gradio as gr\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import builtins\n",
    "import sys\n",
    "\n",
    "# Numerical libs\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from moviepy.video.io.ImageSequenceClip import ImageSequenceClip\n",
    "from imageio import imsave\n",
    "from torch_mir_eval import bss_eval_sources as tbss_eval_sources\n",
    "import torchaudio.transforms as Transforms\n",
    "\n",
    "#torch librosa tools\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank, STFT, ISTFT, magphase\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "from transformers import RobertaTokenizer\n",
    "import librosa\n",
    "import torchaudio\n",
    "from itertools import repeat\n",
    "\n",
    "# muliprocessing tools\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Subset\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from enum import Enum\n",
    "import shutil\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Our libs\n",
    "from arguments import ArgParser\n",
    "from models import ModelBuilder, activate\n",
    "from models.criterion import L1Loss\n",
    "from utils import AverageMeter, \\\n",
    "    recover_rgb, magnitude2heatmap, \\\n",
    "    istft_reconstruction, warpgrid, \\\n",
    "    combine_video_audio, save_video, makedirs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "best_sdr = -1000\n",
    "\n",
    "def realimag(mag, cos, sin):\n",
    "\n",
    "    real = mag * cos\n",
    "    imag = mag * sin\n",
    "\n",
    "    return real, imag\n",
    "\n",
    "\n",
    "# Network wrapper, defines forward pass\n",
    "class NetWrapper(torch.nn.Module):\n",
    "    def __init__(self, nets, args):\n",
    "        super(NetWrapper, self).__init__()\n",
    "        self.net_sound, self.net_clap, self.net_synthesizer = nets\n",
    "\n",
    "        self.N = args.num_mix\n",
    "\n",
    "        self.stft = STFT(n_fft=args.stft_frame, hop_length=args.stft_hop, freeze_parameters=True)\n",
    "        self.istft = ISTFT(n_fft=args.stft_frame, hop_length=args.stft_hop, freeze_parameters=True)\n",
    "\n",
    "        self.src_rate = args.audRate\n",
    "        self.trg_rate = 48000\n",
    "        self.crit = L1Loss()\n",
    "        self.crit2 = nn.CrossEntropyLoss()\n",
    "        self.audLen = args.audLen\n",
    "        self.device = self.net_clap.fc.weight.device\n",
    "\n",
    "    def forward(self, audio, prompt, args):\n",
    "        mix = audio.unsqueeze(0).to(self.device, non_blocking=True)\n",
    "\n",
    "        cond = prompt\n",
    "\n",
    "        real, imag = self.stft(mix)\n",
    "        mag_mix, cos, sin = magphase(real, imag)\n",
    "\n",
    "        # reshape mag_mix\n",
    "        mag_mix = mag_mix.transpose(-1, -2)\n",
    "\n",
    "        B = mag_mix.size(0)\n",
    "        T = mag_mix.size(3)\n",
    "\n",
    "        out_mag_mix = magnitude2heatmap(mag_mix.detach().squeeze().cpu().numpy())[::-1, :, :]\n",
    "\n",
    "        # LOG magnitude\n",
    "        log_mag_mix = torch.log1p(mag_mix).detach()\n",
    "        weight = torch.clamp(log_mag_mix, 1e-3, 10)\n",
    "\n",
    "        text_embed, hidden_state, feat_cond = self.net_clap.get_text_conditioning(cond, tokenize=False, return_hidden_states=True)\n",
    "        feat_cond = activate(feat_cond, args.cond_activation)\n",
    "\n",
    "        feat_sound = self.net_sound(log_mag_mix, hidden_state)\n",
    "        feat_sound = activate(feat_sound, args.sound_activation)\n",
    "\n",
    "        mask = self.net_synthesizer(feat_cond, feat_sound)\n",
    "        mask = activate(mask, args.output_activation)\n",
    "\n",
    "        pred_mask = mask\n",
    "\n",
    "        #aggregate the mag_mix for reconstruction\n",
    "        pred_mag = mag_mix * pred_mask\n",
    "\n",
    "        out_pred_mag = magnitude2heatmap(pred_mag.detach().squeeze().cpu().numpy())[::-1, :, :]\n",
    "\n",
    "        pred_mag = pred_mag.transpose(-1, -2)\n",
    "        real, imag = realimag(pred_mag, cos, sin)\n",
    "        pred_wav = self.istft(real, imag, self.audLen)\n",
    "        out_wav = pred_wav\n",
    "\n",
    "        return [out_wav[0].cpu().numpy(), out_mag_mix, out_pred_mag]\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = ArgParser()\n",
    "    args = parser.parse_train_arguments()\n",
    "\n",
    "    ## fixed the seed\n",
    "    random.seed(args.seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    args.distributed = args.multiprocessing_distributed\n",
    "\n",
    "    ngpus_per_node = args.ngpu if args.ngpu else torch.cuda.device_count()\n",
    "\n",
    "    if args.multiprocessing_distributed:\n",
    "        # Since we have ngpus_per_node processes per node, the total world_size\n",
    "        # needs to be adjusted accordingly\n",
    "        args.world_size = ngpus_per_node * args.world_size\n",
    "        # Use torch.multiprocessing.spawn to launch distributed processes: the\n",
    "        # main_worker process function\n",
    "        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
    "    else:\n",
    "        # Simply call main_worker function\n",
    "        main_worker(args.gpu, ngpus_per_node, args)\n",
    "\n",
    "\n",
    "\n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    args.gpu = gpu\n",
    "    if args.gpu is not None:\n",
    "        print(\"Use GPU: {} for Demo\".format(args.gpu))\n",
    "\n",
    "    if args.multiprocessing_distributed:\n",
    "        if args.dist_url == \"env://\" and args.rank == -1:\n",
    "            args.rank = int(os.environ[\"RANK\"])\n",
    "        if args.multiprocessing_distributed:\n",
    "            # For multiprocessing distributed training, rank needs to be the\n",
    "            # global rank among all the processes\n",
    "            args.rank = args.rank * ngpus_per_node + gpu\n",
    "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                world_size=args.world_size, rank=args.rank)\n",
    "        dist.barrier()\n",
    "\n",
    "    # Network Builders\n",
    "    builder = ModelBuilder()\n",
    "    device = torch.device('cuda:{}'.format(args.gpu))\n",
    "\n",
    "    net_sound = builder.build_sound_net(\n",
    "        fc_dim=args.num_channels,\n",
    "        args=args)\n",
    "\n",
    "    # net_clap = None\n",
    "    net_clap = builder.build_custom_clap(\n",
    "        enable_fusion=False,\n",
    "        device=device,\n",
    "        amodel='HTSAT-base',\n",
    "        tmodel='roberta',\n",
    "        channels=args.num_channels)\n",
    "\n",
    "    net_synthesizer = builder.build_synthesizer(\n",
    "        fc_dim=args.num_channels)\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    ## counting parameters\n",
    "    sound_params = sum(p.numel() for p in net_sound.parameters())\n",
    "    synth_params = sum(p.numel() for p in net_synthesizer.parameters())\n",
    "    clap_params = sum(p.numel() for p in net_clap.parameters())\n",
    "\n",
    "    print(f\"Total parameters of Unet: {sound_params/1000000 : 6.3f}M\")\n",
    "    print(f\"Total parameters of Synthesizer: {synth_params/1000000 : .4e}M\")\n",
    "    print(f\"Total parameters of clap: {clap_params/1000000 : 6.3f}M\")\n",
    "\n",
    "    if not torch.cuda.is_available() and not torch.backends.mps.is_available():\n",
    "        print('using CPU, this will be slow')\n",
    "    elif args.multiprocessing_distributed:\n",
    "        if torch.cuda.is_available():\n",
    "            if args.gpu is not None:\n",
    "                args.batch_size = int(args.batch_size / ngpus_per_node)\n",
    "                args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
    "\n",
    "                torch.cuda.set_device(args.gpu)\n",
    "                net_sound.cuda(args.gpu)\n",
    "                net_clap.cuda(args.gpu)\n",
    "                net_synthesizer.cuda(args.gpu)\n",
    "\n",
    "                nets = (net_sound, net_clap, net_synthesizer)\n",
    "\n",
    "                # Wrap networks\n",
    "                model = NetWrapper(nets, args)\n",
    "                model.cuda(args.gpu)\n",
    "\n",
    "                model_params = sum(p.numel() for p in model.parameters())\n",
    "                train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                print(f\"Total parameters of Model: {model_params/1000000 : 6.3f}M\")\n",
    "                print(f\"Total trainable parameters: {train_params/1000000 : 6.3f}M\")\n",
    "\n",
    "                model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True)\n",
    "\n",
    "            else:\n",
    "                model.cuda()\n",
    "                model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "    elif args.gpu is not None and torch.cuda.is_available():\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        model = model.cuda(args.gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    if args.load:\n",
    "        if os.path.isfile(args.load):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.load))\n",
    "            if args.gpu is None:\n",
    "                checkpoint = torch.load(args.load)\n",
    "            elif torch.cuda.is_available():\n",
    "                # Map model to be loaded to specified single gpu.\n",
    "                loc = 'cuda:{}'.format(args.gpu)\n",
    "                checkpoint = torch.load(args.load, map_location=loc)\n",
    "\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.load, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.load))\n",
    "\n",
    "\n",
    "    def separate_audio(audio, prompt):\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "        tokenize_prompt = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # process the audio \n",
    "        (sr, src_audio) = audio\n",
    "        dtype = src_audio.dtype\n",
    "\n",
    "        if src_audio.dtype == 'int16':\n",
    "            src_audio = src_audio / 2 ** 15\n",
    "\n",
    "        if src_audio.ndim == 2:\n",
    "            if dtype == 'int16':\n",
    "                src_audio = librosa.to_mono(src_audio.transpose(-1, -2).astype(np.float32))\n",
    "            else:\n",
    "                src_audio = librosa.to_mono(src_audio.astype(np.float32))\n",
    "\n",
    "        #resample audio        \n",
    "        if sr != SR_TRG:\n",
    "            src_audio = librosa.resample(src_audio.astype(np.float32), orig_sr=sr, target_sr=SR_TRG).astype(np.float32)\n",
    "            sr = SR_TRG\n",
    "\n",
    "        src_audio[src_audio > 1.] = 1.\n",
    "        src_audio[src_audio < -1.] = -1.\n",
    "\n",
    "        # padding src_audio\n",
    "        len_src = len(src_audio)\n",
    "        r = 131070 - len_src % 131070\n",
    "        src_audio = np.concatenate([src_audio, np.zeros(r)])\n",
    "        output_audio = np.zeros_like(src_audio)\n",
    "        len_src = len(src_audio)\n",
    "        duration = len_src // SR_TRG\n",
    "\n",
    "        start = 0\n",
    "        src_specs = []\n",
    "        output_specs = []\n",
    "\n",
    "        while start < len_src:\n",
    "            sample = src_audio[start : start + 131070]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output, spec_in, spec_out = model(torch.tensor(sample.astype(np.float32)), tokenize_prompt, args)\n",
    "                src_specs.append(spec_in)\n",
    "                output_specs.append(spec_out)\n",
    "\n",
    "            output_audio[start : start + 131070] = output\n",
    "\n",
    "            start += 131070\n",
    "\n",
    "        output_audio = output_audio[:-(r)]\n",
    "\n",
    "        if len(output_specs) > 1:\n",
    "            output_spec = np.concatenate(output_specs, axis=1)\n",
    "            src_spec = np.concatenate(src_specs, axis=1)\n",
    "\n",
    "        else:\n",
    "            output_spec = output_specs[-1]\n",
    "            src_spec = src_specs[-1]\n",
    "\n",
    "        rI = int(r * 512 / 131070)\n",
    "\n",
    "        output_spec = output_spec[:, :-rI, :]\n",
    "        src_spec = src_spec[:, :-rI, :]\n",
    "\n",
    "        return src_spec, output_spec, (SR_TRG, output_audio)\n",
    "\n",
    "\n",
    "    def launch_gradio():\n",
    "        with gr.Blocks() as demo:\n",
    "            gr.Markdown(\"Separate audiofrom mixtures using text queries with this demo.\")\n",
    "\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    input_audio = gr.Audio()\n",
    "                    input_text =  gr.Textbox(placeholder=\"Provide the text prompt\")\n",
    "                    button = gr.Button(\"Submit\")\n",
    "\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        input_spec = gr.Image(label=\"Mixture Spectrogram\")\n",
    "                        output_spec = gr.Image(label=\"Separated Spectrogram\")\n",
    "\n",
    "                    output_audio = gr.Audio()\n",
    "\n",
    "            button.click(separate_audio, inputs=[input_audio, input_text], outputs=[input_spec, output_spec, output_audio])\n",
    "            examples = gr.Examples(examples=[\n",
    "                [f\"{args.samples_dir}/sample1.wav\", \"a man is talking, clean sound with no background noises\"],\n",
    "                [f\"{args.samples_dir}/sample2.wav\", \"A woman speaks, clean sound with no background noises\"],\n",
    "                [f\"{args.samples_dir}/sample2.wav\", \"A cat meows in a silent room, clean sound with no background noises\"],\n",
    "                [f\"{args.samples_dir}/sample2.wav\", \"A girl is coughing in a silent room, clean sound with no other noises\"]\n",
    "            ],\n",
    "                inputs=[input_audio, input_text])\n",
    "\n",
    "        demo.launch(share=True)\n",
    "\n",
    "\n",
    "    SR_TRG = args.audRate\n",
    "\n",
    "    launch_gradio()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
